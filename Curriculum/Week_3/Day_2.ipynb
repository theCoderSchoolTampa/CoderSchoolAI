{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "from torch import nn, optim, functional\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train_mnist(model, epochs=5, test_every_ep=90, learning_rate=0.003):\n",
    "\n",
    "    # This makes our data easier to work with for the Network\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Download/Load Data from Online Hub\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    trainloader = th.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Same for Testing Data\n",
    "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "    testloader = th.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "    test_iter = iter(testloader)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        step = 0   \n",
    "        for images, labels in trainloader:\n",
    "            # Flatten the images\n",
    "            step+=1\n",
    "            labels = th.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 30 == 0:\n",
    "                print('Loss:', loss.item())\n",
    "            running_loss += loss.item()\n",
    "            if step % test_every_ep == 0:\n",
    "                model.eval()\n",
    "                with th.no_grad(): # Lets Perform a Check to see how Accurate our model is\n",
    "                    image, labels = next(test_iter) # Get next Training Set\n",
    "                    labels_one_hot = th.nn.functional.one_hot(labels, num_classes=10).float() # Get MNIST labels for the Training Set\n",
    "                    output = model(image) # Perform Forward Pass\n",
    "                    loss = criterion(output, labels_one_hot) # Grade the Output\n",
    "                    predictions = th.argmax(output, dim=1) # Check to see what it predicted\n",
    "                    correct = (predictions == labels).sum().item() # See how many it got right\n",
    "                    total = labels.size(0) \n",
    "                print(\"Accuracy: {:.2f}%\".format(correct/total*100))\n",
    "                print('Testing Loss:', loss.item())\n",
    "        print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(trainloader)}\")\n",
    "    print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m input_block \u001b[38;5;241m=\u001b[39m InputBlock(in_attribute\u001b[38;5;241m=\u001b[39mimage, is_module_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define the ConvBlock which acts as a convolutional layer for processing the game state.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# The depth of 3 represents the number of convolutional layers in this block.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m conv_block \u001b[38;5;241m=\u001b[39m \u001b[43mConvBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_attribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m lin_block \u001b[38;5;241m=\u001b[39m LinearBlock(input_size\u001b[38;5;241m=\u001b[39mconv_block\u001b[38;5;241m.\u001b[39moutput_size, output_size\u001b[38;5;241m=\u001b[39mconv_block\u001b[38;5;241m.\u001b[39moutput_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39mconv_block\u001b[38;5;241m.\u001b[39moutput_size, num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Define the OutputBlock that will decide the next action to take based on the current game state.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# The num_classes corresponds to the number of possible actions the snake can take (up, down, left, right).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\Blocks\\ConvBlock.py:46\u001b[0m, in \u001b[0;36mConvBlock.__init__\u001b[1;34m(self, input_shape, num_channels, depth, disable_max_pool, desired_output_size, activation, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_max_pool \u001b[38;5;241m=\u001b[39m disable_max_pool\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregenerate_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\Blocks\\ConvBlock.py:133\u001b[0m, in \u001b[0;36mConvBlock.regenerate_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m x \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m--> 133\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_desired_output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Environment.Attributes import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "from CoderSchoolAI.Training.Datasets import train_on_dataset, MNISTDataset\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "image = ObsAttribute(name=\"img\", space=BoxType(-1, 1, shape=(1, 28, 28)))\n",
    "input_block = InputBlock(in_attribute=image, is_module_dict=False, device=device)\n",
    "\n",
    "# Define the ConvBlock which acts as a convolutional layer for processing the game state.\n",
    "# The depth of 3 represents the number of convolutional layers in this block.\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape, num_channels=1, depth=4, device=device)\n",
    "lin_block = LinearBlock(input_size=conv_block.output_size, output_size=conv_block.output_size/2, hidden_size=conv_block.output_size, num_hidden_layers=3, dropout=0.2, device=device)\n",
    "# Define the OutputBlock that will decide the next action to take based on the current game state.\n",
    "# The num_classes corresponds to the number of possible actions the snake can take (up, down, left, right).\n",
    "out_block = OutputBlock(input_size=conv_block.output_size, num_classes=10, device=device)\n",
    "\n",
    "# Initialize the network and add the blocks\n",
    "net = Net(device='cuda')\n",
    "\n",
    "net.add_block(input_block)\n",
    "net.add_block(conv_block)\n",
    "net.add_block(out_block)\n",
    "net.compile()\n",
    "\n",
    "print(out_block.device)\n",
    "# Call the function to train the model\n",
    "mnist_dataset = MNISTDataset()\n",
    "train_on_dataset(net, mnist_dataset, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

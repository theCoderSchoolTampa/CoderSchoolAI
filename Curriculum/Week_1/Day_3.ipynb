{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Day 3:**\n",
    "We Will be introducing the main topics of this camp, Agent Artificial Intelligence (AI)**\n",
    "\n",
    "![Agent AI](https://lilianweng.github.io/posts/2018-02-19-rl-overview/RL_illustration.png)\n",
    "\n",
    "## What is Agent Learning?\n",
    "\n",
    "*\"Try something new, add randomness to your actions. Then, compare the result to your expectation. If the result suprises you, maybe exceeded your expectations, then change your parameters to increase taking those actions in the future.\" ~ Ilya Sutskever*\n",
    "\n",
    "### **What Does this Even Mean?**\n",
    "\n",
    "1. \"Try something new, add randomness to your actions.\"\n",
    "\n",
    "This is like trying a new type of ice cream flavor instead of always sticking to vanilla. You never know, you might find a new favorite! This is what we call 'exploration' in reinforcement learning, where an AI agent tries different actions to see what happens. It's like a robot exploring a new planet.\n",
    "\n",
    "\n",
    "2. \"Then, compare the result to your expectation.\"\n",
    "\n",
    "After you've tried the new ice cream flavor, you think about whether it was better, worse, or just as you expected. In reinforcement learning, this is like the AI agent checking the reward it gets after taking an action. If the ice cream was yummy, it's like getting a good reward!\n",
    "\n",
    "\n",
    "3. \"If the result surprises you, maybe exceeded your expectations...\"\n",
    "\n",
    "Sometimes you might be surprised by how much you liked the new flavor. Maybe you expected it to be just okay, but it was actually delicious! In reinforcement learning, this is like getting a higher reward than expected. It's like if the robot found a shiny gem on the planet when it was only expecting to find rocks.\n",
    "\n",
    "\n",
    "4. \"...then change your parameters to increase taking that action in the future.\"\n",
    "\n",
    "If you really liked the new flavor, you might decide to choose it more often in the future. You changed your 'ice cream picking rule' based on the new information. In reinforcement learning, this is called 'exploitation'. The AI agent adjusts its policy (which is like its 'rule book' for picking actions) to do actions that give good rewards more often. It's like the robot deciding to look for shiny gems more often, because it learned that finding gems is better than finding rocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoderSchoolAI # Imports the entire CoderSchoolAI library!\n",
    "from CoderSchoolAI import * # Imports all of the CoderSchoolAI library's things! Think of sprinkles and Cake Batter!\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import * # We are going to use a pre-cooked Cake from the Library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(snake_env, steps=10000, save_file=\"./QSnakeAgent.pkl\", log_interval=1000):\n",
    "    s = 0\n",
    "    while s < steps:\n",
    "        snake_env.update_env() # Update the environment in what we call a loop.\n",
    "        s+=1\n",
    "    snake_env.snake_agent.qlearning.save_q_table(save_file)\n",
    "    \n",
    "def load(snake_env, steps=10000, save_file=\"./QSnakeAgent.pkl\"):\n",
    "    s = 0\n",
    "    snake_env.snake_agent.qlearning.load_q_table(save_file)\n",
    "    snake_env.snake_agent.qlearning.epsilon = 0\n",
    "    while s < steps:\n",
    "        snake_env.update_env() # Update the environment in what we call a loop.\n",
    "        s+=1\n",
    "snake_env = SnakeEnv(\n",
    "    target_fps=6, \n",
    "    height=8,\n",
    "    width=8,\n",
    "    cell_size=80,\n",
    "    is_user_control=False, \n",
    "    snake_is_q_table=True,\n",
    "    verbose=True,\n",
    "    policy_kwargs=dict(\n",
    "        alpha=0.9, \n",
    "        gamma=0.85,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.999,\n",
    "        )\n",
    "\n",
    "                     ) # Create a SnakeEnv object!\n",
    "snake_env.reset() # Reset the environment!\n",
    "learn(snake_env, steps=1000000, save_file=\"./QSnakeAgent.pkl\")\n",
    "while True: # Loop until the game is over.\n",
    "    snake_env.update_env() # Update the environment in what we call a loop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trial and Error**: \n",
    "Q-Learning is all about an agent learning from its actions (trial) and their outcomes (errors). The agent interacts with an environment by taking actions, and it learns from the rewards (positive or negative) it gets.\n",
    "\n",
    "\n",
    "- **Value of Actions (Q-value)**: \n",
    "Q-Learning estimates the value (or \"Q-value\") of taking a certain action in a certain state. This is an estimate of reward that the agent can expect to receive if it takes that action in that state, considering both immediate and future rewards.\n",
    "\n",
    "\n",
    "- **Exploration vs Exploitation**: \n",
    "Q-Learning involves a balance between exploration (trying out new actions to see their outcomes) and exploitation (choosing the action that the agent currently believes to have the best outcome). \n",
    "This is often handled using an \"*epsilon-greedy*\" strategy, where the agent randomly explores with a probability of epsilon, and exploits its current knowledge with a probability of 1 - epsilon. \n",
    "This just means that sometimes it will explore new actions, and then other times it will use its knowledge of the best action.\n",
    "\n",
    "\n",
    "- **Learning Rate (Alpha)**: The learning rate alpha determines how much the Q-values are updated at each step. If alpha is 1, the agent completely trusts the new information it gets. If alpha is 0, the agent doesn't learn anything new.\n",
    "\n",
    "\n",
    "- **Discount Factor (Gamma)**: The discount factor gamma decides how much importance to give to future rewards compared to immediate rewards. If gamma is 0, the agent only cares about immediate rewards. If gamma is close to 1, the agent cares a lot about maximizing future rewards.\n",
    "\n",
    "\n",
    "- **Q-Table**: \n",
    "The Q-values are usually stored in a table called the Q-table. Each entry in the Q-table corresponds to a certain state-action pair. The agent uses this table as a \"cheat sheet\" to decide what action to take in a given state. \n",
    "This can be thought of your memory through a game, where keeping track of the games history will let us be able to remember what to do next.\n",
    "\n",
    "\n",
    "- **Update Rule**: At each step, the Q-values are updated using the Q-Learning update rule (also called the Temporal Difference (TD) update rule). This rule is a kind of \"weighted average\" of the old Q-value and the newly estimated Q-value.\n",
    "\n",
    "\n",
    "- **Convergence**: \n",
    "Over time, as the agent explores the environment enough and updates the Q-table, the Memory in the table will tell the Player exactly how to act in any scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

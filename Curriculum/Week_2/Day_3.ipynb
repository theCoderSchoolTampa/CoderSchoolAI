{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks for the Snake Game\n",
    "\n",
    "This tutorial aims to cover the essentials of building a neural network to play the snake game. The main points are: understanding neural networks, understanding the data, and using the CoderSchoolAI.Neural library.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Neural Networks are a set of algorithms modeled after the human brain. They are designed to recognize patterns, which makes them useful in machine learning and artificial intelligence applications.\n",
    "\n",
    "In the context of our Snake game, we'll be building a neural network to make decisions based on the game's current state. Given the current state, our network will be responsible for deciding which move (up, down, left, right) will maximize the chances of winning the game.\n",
    "\n",
    "## Understanding the Data\n",
    "\n",
    "For the Snake game, our data comes in the form of the game state, actions, and reward:\n",
    "\n",
    "1. **State**: The state of the game is like a snapshot of what the game looks like right now. In Snake, the state might be the position of the snake and the position of the apple.\n",
    "\n",
    "2. **Actions**: Actions are the things you can do in the game. In Snake, the actions would be moving up, down, left, or right.\n",
    "\n",
    "3. **Reward**: The reward is what the game gets for performing an action. In Snake, the reward might be the points received for eating an apple or a penalty for crashing into a wall or the snake itself.\n",
    "\n",
    "## CoderSchoolAI.Neural Library\n",
    "\n",
    "This library provides tools to easily build and manipulate deep neural networks. We will be using the Net class to build our neural network.\n",
    "\n",
    "Here is a brief overview of the classes and methods provided by the library:\n",
    "\n",
    "### Net Class\n",
    "\n",
    "This is the main class that will be used to build the network structure. It has the following methods:\n",
    "\n",
    "- `__init__(self, network_name:str= \"basic_net\", is_dict_network=False, device: th.device = th.device('cpu'))`: This initializes the Net. It can be named for organization, declared as a dictionary network, and assigned a specific device for computation.\n",
    "\n",
    "- `add_block(self, block: Block)`: Adds a Block connection to the network. The type of Block is based on the input.\n",
    "\n",
    "- `compile(self)`: Once all blocks have been added, this function will compile the network and prepare it for training.\n",
    "\n",
    "- `forward(self, x: th.Tensor)`: This function is called during training to pass the input data through the network.\n",
    "\n",
    "- `copy(self)`: This function copies the current network, useful for creating multiple instances of the same network.\n",
    "\n",
    "### InputBlock Class\n",
    "\n",
    "The InputBlock is the first layer of the network, it takes in the input data.\n",
    "\n",
    "- `__init__(self, in_attribute:Union[Attribute, Dict[str, Attribute]], is_module_dict:bool=False, device: th.device = th.device('cpu'))`: Initializes the InputBlock with attributes, declares if it is a module dictionary, and assigns the computational device.\n",
    "\n",
    "- `forward(self, x) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "### LinearBlock Class\n",
    "\n",
    "The LinearBlock is a fully connected layer of the network (MLP).\n",
    "\n",
    "- `__init__(self, input_size: int, output_size: int, num_hidden_layers: int = 3, hidden_size: Union[int, List[int]] = 128, activation: Optional[Callable] = None, device: th.device = th.device('cpu'))`: Initializes the LinearBlock with the input size, output size, number of hidden layers, size of hidden layers, an optional activation function, and the device for computation.\n",
    "\n",
    "- `forward(self, x: th.Tensor) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "### ConvBlock Class\n",
    "\n",
    "The ConvBlock class is for building convolutional neural network (CNN) architectures.\n",
    "\n",
    "Below is an explanation of the main methods within this class:\n",
    "\n",
    "- `__init__(self, input_shape: int, num_channels: int, depth: int = 3, disable_max_pool: bool = False, desired_output_size: int = None, activation: Optional[Callable] = None, device: th.device = th.device('cpu'))`: Initializes the ConvBlock with various parameters including input shape, number of channels, depth (i.e. the number of convolutional layers), and optional parameters such as max pooling disablement, desired output size, and activation function. The computational device can also be set.\n",
    "\n",
    "- `forward(self, x) -> th.Tensor`: This function is called during training to pass the input data through the block. It first checks the input shape to ensure it matches the expected input shape for the ConvBlock, and raises an error if the shape doesn't match. It then passes the input through the block's module (which is a sequence of convolutional, activation and pooling layers) to generate the output. If the block has forward connections, it calls the forward method on the next block in the sequence.\n",
    "\n",
    "### OutputBlock Class\n",
    "\n",
    "The OutputBlock is the final layer of the network which outputs the results.\n",
    "\n",
    "- `__init__(self, input_size, num_classes, device: th.device = th.device('cpu'))`: Initializes the OutputBlock some specified input size to output some number of classes.\n",
    "\n",
    "- `forward(self, x: th.Tensor) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "## Putting it all Together\n",
    "\n",
    "To put all of this together for our Snake game, we will need to create a neural network that can take the game's current state and output a decision for the next action. \n",
    "\n",
    "The basic steps are as follows:\n",
    "\n",
    "1. **Create InputBlock**: We'll need an InputBlock to take in our game state. The state can be an array containing information about the snake's position, the apple's position, and any other relevant game information.\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "```\n",
    "\n",
    "2. **Create Hidden Layers (LinearBlocks)**: These layers will do the computation based on our input. The exact number and size of these layers can vary depending on the complexity of the game and the sophistication of the strategy you want to develop.\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "    lin_block = LinearBlock(input_size=len(snake_env.snake_agent.get_q_table_state()), num_hidden_layers=3, hidden_size=128, dropout=0.05)\n",
    "```\n",
    "\n",
    "3. **Create Image Filter Blocks (ConvBlocks)**: This is a basic architecture commonly used for any image processing tasks. Our SnakeEnv has a GameState which just so happends to be an Image!\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "    conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=3)\n",
    "```\n",
    "\n",
    "3. **Create OutputBlock**: Finally, we need an OutputBlock that will output the decision for the next move. This will usually be a single value indicating the direction to move (up, down, left, right).\n",
    "\n",
    "```python\n",
    "    # Output Example with ConvBlocks!\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "    conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=3)\n",
    "    OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "```\n",
    "\n",
    "```python\n",
    "    # Output Example with LinearBlocks!\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "    lin_block = LinearBlock(input_size=len(snake_env.snake_agent.get_q_table_state()), num_hidden_layers=3, hidden_size=128, dropout=0.05)\n",
    "    OutputBlock(input_size=lin_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "```\n",
    "\n",
    "4. **Compile the Network**: Once all blocks have been added, compile the network with `compile()` method of the Net class.\n",
    "\n",
    "```python\n",
    "    # Build the Network from Blocks\n",
    "    net = Net()\n",
    "    net.add_block(input_block)\n",
    "    net.add_block(conv_block)\n",
    "    net.add_block(out_block)\n",
    "\n",
    "    # Compile the Network!\n",
    "    net.compile()\n",
    "```\n",
    "\n",
    "\n",
    "5. **Train the Network**: After compiling the network, we'll need to train it. This involves playing the game many times and adjusting the weights of the network based on the outcomes. This process is usually facilitated by reinforcement learning algorithms.\n",
    "\n",
    "\n",
    "    `Think about training a computer to play a video game. It's a lot like teaching a dog a new trick: we want the computer to learn how to do something really well.`\n",
    "\n",
    "- **The Game**: `In the game, our computer (which we'll call the 'agent') is going to play   many rounds (or 'episodes') of the game. In each round, it will try different things to see what works.`\n",
    "\n",
    "- **Deep Q-Learning**:` We're using a special training method called 'Deep Q-Learning.'     This is a set of rules that tells the agent how to play and learn from the game. It's kind  of like the rule book for the agent.`\n",
    "\n",
    "- **Epsilon-Greedy Strategy**: `To start, the agent mostly guesses what to do (we call  these 'actions'). But, over time, it starts to learn which actions work best. It still   guesses sometimes, though, because that could lead to even better actions. We call this the 'Epsilon-Greedy Strategy.' `\n",
    "\n",
    "- **Memory**: `Just like how you remember your best games, the agent also stores its    experiences (the game state, its actions, and rewards) in its memory. This helps it learn  over time.`\n",
    "\n",
    "- **Learning**:` After each episode, the agent takes a look at its memory and learns from   it. It then adjusts its playing strategy based on what it learned.`\n",
    "\n",
    "- **Playing the Game**: `The agent keeps playing the game, adjusting its strategy, and  filling its memory. It does this for a lot of episodes, and with each episode, it should   get a little better!`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a great example, and indeed shows the use of the `CoderSchoolAI.Neural` library in a concise manner. I'll extend this example with more comments for additional clarity:\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary modules\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th\n",
    "\n",
    "# Initialize the game environment with a 16x16 grid\n",
    "snake_env = SnakeEnv(width=8, height=8)\n",
    "\n",
    "# Define the InputBlock. \n",
    "# We use the game state attribute from the environment as the input to our network.\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "\n",
    "# Define the ConvBlock which acts as a convolutional layer for processing the game state.\n",
    "# The depth of 3 represents the number of convolutional layers in this block.\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape, num_channels=1, depth=3)\n",
    "\n",
    "# Define the OutputBlock that will decide the next action to take based on the current game state.\n",
    "# The num_classes corresponds to the number of possible actions the snake can take (up, down, left, right).\n",
    "# out_block = OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "\n",
    "# Initialize the network and add the blocks\n",
    "net = Net()\n",
    "net.add_block(input_block)\n",
    "net.add_block(conv_block)\n",
    "# net.add_block(out_block)\n",
    "\n",
    "# Compile the network\n",
    "net.compile()\n",
    "\n",
    "# Test the network\n",
    "# We get a sample game state and feed it through the network.\n",
    "input_sample = snake_env.get_attribute(\"game_state\").sample()\n",
    "output_test = net(input_sample)\n",
    "\n",
    "# Test network copying\n",
    "# We create a copy of the network and test it with the same input sample.\n",
    "# This is to verify that the copying operation works correctly.\n",
    "copy_of_net = net.copy()\n",
    "output_copy_test = copy_of_net(input_sample)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "The performance of the neural network would depend on the exact architecture (number and type of blocks), as well as the training process, which has not been covered in this code snippet. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoderSchoolAI v0.0.9: \n",
      "CoderSchoolAI: A Python Module designed for teaching Modern Day AI to Kids.\n",
      "\n",
      "This package includes a range of educational tools and templates designed to simplify complex concepts and offer improved learning opportunities. \n",
      "It enables the exploration of foundational principles of problem-solving through Artificial Intelligence, providing structured guidance for the exploration and \n",
      "implementation of theoretical concepts.\n",
      "\n",
      "Key Features:\n",
      " - Learning Curriculum: Our module makes learning programming engaging and fun, turning complex ideas into digestible chunks.\n",
      " \n",
      " - Educational Tools and Templates: We provide tools and templates to simplify complex concepts and enhance learning opportunities.\n",
      " \n",
      " - Exploration of Foundational Concepts: Our module enables the exploration of foundational principles of problem-solving through Artificial Intelligence.\n",
      " - Structural Guidance: We offer structured guidance for exploring and implementing theoretical concepts in a hands-on way.\n",
      " \n",
      " - Simplified Machine Learning Principles: Our module breaks down complex machine learning  principles to facilitate understanding and utilization of these fundamental mechanisms.\n",
      " \n",
      " - Neural Networks to Neural Blocks: Our module implements a simpler concept called neural blocks. A Neural Block is an auto-generated neural network Sequential that can be trained to learn from data. \n",
      " \n",
      " - Training Agents/Bots: Users can train agents or bots to play games, leveraging principles of data engineering and neural networks.\n",
      " \n",
      "Algorithms Available:\n",
      " - Deep-Q w/ Target Network: Deep-Q is a deep reinforcement learning algorithm that uses a target network\n",
      " \n",
      "The goal of this module is to make Agent AI and machine learning more accessible and enjoyable to learn.\n",
      "\n",
      "Future Enhancements:\n",
      " - Adding {Join/Split/Add} support for Creating Network Chains.\n",
      " - Adding Support for Neural Block Graph Architectures.\n",
      " - Support for multiple agents/bots in a single game.\n",
      " - PPO, DDPG, SAC, TD3, PPO2, DDPG2, SAC2, TRPO\n",
      "\n",
      "Known Issues: \n",
      "\n",
      "Development Environment: \n",
      " - Python 3.6+\n",
      "\n",
      "Contribution Guidelines:\n",
      " - Pull from GitHub: https://github.com/theCoderSchoolTampa/CoderSchoolAI/CoderSchoolAI/blob/master/\n",
      " \n",
      " - Make Desired Changes to source code\n",
      " \n",
      " - Document Changelog: \n",
      "        Format [Name] - [Date] - [PR #]:\n",
      "        - [Change]: \n",
      "            [Description]\n",
      "        - [Change]:\n",
      "            [Description]\n",
      "            \n",
      " - Edit README.md Documentation if Applicable\n",
      " \n",
      " - Create a Pull Request: https://github.com/theCoderSchoolTampa/CoderSchoolAI/CoderSchoolAI/pulls/\n",
      " \n",
      " - Grab a Cup o' Joe, and know that I appreciate you for contributing work to this project :D\n",
      "\n",
      "\n",
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Network...\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.InputBlock.InputBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.FlattenBlock.FlattenBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.LinearBlock.LinearBlock>,\n",
      "  (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.OutputBlock.OutputBlock>,\n",
      "  (0): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (1): Identity()\n",
      "\n",
      "Compiling Network...\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.InputBlock.InputBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.FlattenBlock.FlattenBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.LinearBlock.LinearBlock>,\n",
      "  (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.OutputBlock.OutputBlock>,\n",
      "  (0): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (1): Identity()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### How do We use this in Our Case?\n",
    "\"\"\"\n",
    "Here is the Basic template for Building a Neural Network for our Snake!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.FrozenLakeEnvironment import *\n",
    "\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "from CoderSchoolAI.Neural.ActorCritic.ActorCriticNetwork import *\n",
    "import torch as th\n",
    "\n",
    "# snake_env = SnakeEnv(width=8, height=8)\n",
    "frozen_lake_env = FrozenLakeEnv()\n",
    "input_block = InputBlock(in_attribute=frozen_lake_env.get_attribute(\"game_state\"), is_module_dict=False,)\n",
    "# conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "flatten_size = np.prod(frozen_lake_env.get_attribute(\"game_state\").shape)\n",
    "flat_block = FlattenBlock(flatten_size)\n",
    "lin_block = LinearBlock(flat_block.output_size, 16, num_hidden_layers=2, hidden_size=[32, 32])\n",
    "\n",
    "out_block = OutputBlock(input_size=lin_block.output_size, num_classes=len(frozen_lake_env.agent.get_actions()),)\n",
    "\n",
    "q_net = Net(name='test_ppo_net_with_game_state')\n",
    "q_net.add_block(input_block)\n",
    "q_net.add_block(flat_block)\n",
    "q_net.add_block(lin_block)\n",
    "q_net.add_block(out_block)\n",
    "q_net.compile()\n",
    "\n",
    "copy_net = q_net.copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`deep_q_learning` function:\n",
    "\n",
    "- **agent**: This is our player or the character in the game. \n",
    "\n",
    "- **environment**: This is the game world or the level where our character is playing.\n",
    "\n",
    "- **q_network**: Think of this as the brain of our character. It's what helps the character make decisions based on what it has learned.\n",
    "\n",
    "- **target_q_network**: This is a copy of the character's brain, but we only update it from time to time. We use it to help the character make more stable decisions.\n",
    "\n",
    "- **buffer**: This is the character's memory. It stores what the character has seen, what it did, and what happened.\n",
    "\n",
    "- **num_episodes**: This is how many rounds of the game the character will play. More rounds mean more practice!\n",
    "\n",
    "- **max_steps_per_episode**: This is the maximum number of moves the character can make in one round. If a round takes too long, we stop it.\n",
    "\n",
    "- **gamma**: This is a measure of how much our character cares about future rewards compared to immediate ones. A high value makes the character care more about long-term results.\n",
    "\n",
    "- **update_target_every**: This tells us how often we update the copy of the character's brain. For example, if it's set to 10, we update it after every 10 rounds.\n",
    "\n",
    "- **batch_size**: This is how many memories we use when teaching the character from its past experiences.\n",
    "\n",
    "- **epsilon**: This is how often the character makes random moves. At first, we want the character to explore a lot and try different things, so we set this high.\n",
    "\n",
    "- **epsilon_decay**: This is how quickly we reduce the amount of randomness in the character's actions. As the character learns more, we want it to rely less on random moves.\n",
    "\n",
    "- **stop_epsilon**: This is the lowest amount of randomness we allow in the character's actions. We always want it to try something new from time to time!\n",
    "\n",
    "- **alpha**: This is how quickly the character learns. A high value means the character learns quickly but might miss some details. A low value means slower learning, but more attention to details.\n",
    "\n",
    "- **attributes**: These are extra details or features that the character can use to make decisions. For example, it could be the character's health or number of lives in the game.\n",
    "\n",
    "- **optimizer**: This is a tool that helps the character's brain learn from its mistakes and get better.\n",
    "\n",
    "- **fps**: This is how fast the game runs. A higher number means the game runs faster.\n",
    "\n",
    "Playing around with these parameters changes how the character learns and plays the game. So go ahead and tweak them to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:102: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  state_tensor = th.tensor(state, dtype=th.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive\\theCoderSchool\\CoderSchoolAI\\Curriculum\\Week_2\\Day_3.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCoderSchoolAI\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mEnvironment\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mAgent\u001b[39;00m \u001b[39mimport\u001b[39;00m BasicReplayBuffer, DictReplayBuffer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m deep_q_learning(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     agent\u001b[39m=\u001b[39;49mfrozen_lake_env\u001b[39m.\u001b[39;49magent,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     environment\u001b[39m=\u001b[39;49mfrozen_lake_env,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     q_network\u001b[39m=\u001b[39;49mcopy_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     target_q_network\u001b[39m=\u001b[39;49mq_net,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     update_target_every\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     buffer\u001b[39m=\u001b[39;49m BasicReplayBuffer(batch_size),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_steps_per_episode\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     epsilon_decay\u001b[39m=\u001b[39;49m\u001b[39m0.9999\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     stop_epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     reward_norm_coef\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     attributes\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgame_state\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/theCoderSchool/CoderSchoolAI/Curriculum/Week_2/Day_3.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:136\u001b[0m, in \u001b[0;36mdeep_q_learning\u001b[1;34m(agent, environment, q_network, target_q_network, buffer, num_episodes, max_steps_per_episode, gamma, update_target_every, batch_size, epsilon, epsilon_decay, stop_epsilon, alpha, reward_norm_coef, attributes, optimizer, optimizer_kwargs, fps, log_frequency)\u001b[0m\n\u001b[0;32m    133\u001b[0m                                             \u001b[39m# TODO: figure out: Check if the action space is dictionary type ???\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m episode \u001b[39m<\u001b[39m num_episodes\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    135\u001b[0m         \u001b[39m# Collect samples, then perform an update on the Q-network\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m         collect_rollouts()\n\u001b[0;32m    137\u001b[0m         \u001b[39m# Sample a batch from the replay buffer\u001b[39;00m\n\u001b[0;32m    138\u001b[0m         states, actions, _, _, rewards, dones, next_states, batches \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mgenerate_batches()\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:102\u001b[0m, in \u001b[0;36mdeep_q_learning.<locals>.collect_rollouts\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m# Convert state to tensor for feeding into the network\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(buffer, BasicReplayBuffer):\n\u001b[1;32m--> 102\u001b[0m     state_tensor \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39;49mtensor(state, dtype\u001b[39m=\u001b[39;49mth\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m    103\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     state_tensor \u001b[39m=\u001b[39m dict_to_tensor(state, q_network\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "### Running the Training ###\n",
    "\"\"\"\n",
    "Remember:\n",
    "- We can edit parameters for the Learning Algorithm!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Training.Algorithms import deep_q_learning\n",
    "from CoderSchoolAI.Environment.Agent import BasicReplayBuffer, DictReplayBuffer\n",
    "\n",
    "batch_size = 32\n",
    "deep_q_learning(\n",
    "    agent=frozen_lake_env.agent,\n",
    "    environment=frozen_lake_env,\n",
    "    q_network=copy_net,\n",
    "    target_q_network=q_net,\n",
    "    update_target_every=100,\n",
    "    buffer= BasicReplayBuffer(batch_size),\n",
    "    num_episodes=10000,\n",
    "    max_steps_per_episode=100,\n",
    "    epsilon_decay=0.9999,\n",
    "    stop_epsilon=0.1,\n",
    "    batch_size=batch_size,\n",
    "    reward_norm_coef=0.8,\n",
    "    alpha=0.005,\n",
    "    attributes=\"game_state\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the Network ####\n",
    "file_path = \".\"\n",
    "q_net.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How Good Is your Model?\n",
    "from CoderSchoolAI.Environment.Agent import *\n",
    "from CoderSchoolAI.Environment.Shell import *\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th\n",
    "\n",
    "snake_env = SnakeEnv(width=8, height=8, cell_size=60, verbose=True, target_fps=8)\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False,)\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "out_block = OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()),)\n",
    "\n",
    "q_net = Net(name='test_ppo_net_with_game_state')\n",
    "q_net.add_block(input_block)\n",
    "q_net.add_block(conv_block)\n",
    "q_net.add_block(out_block)\n",
    "q_net.compile()\n",
    "q_net.load(\".\") \n",
    "\n",
    "def demo_model(\n",
    "    agent: Agent,  # Actor in the Environment\n",
    "    environment: Shell,  # Environment which the Deep Q Network is being trained on \n",
    "    q_network: Net,  # The pretrained Deep Q Network\n",
    "    num_episodes: int = 10,  # Number of episodes to demonstrate\n",
    "    max_steps_per_episode: int = 100,  # Stops the Epsiodic Sampling when the number of steps per episode reaches this value\n",
    "    attributes: Union[str, Tuple[str]] = None,  # attributes to be used for the Network\n",
    "    fps: int = 10,  # Frames per second to run the Agent\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A demonstration of Deep Q Learning with a pretrained Deep Q Network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the Q-network is in evaluation mode\n",
    "    q_network.eval()\n",
    "\n",
    "    # Check if the action space is dictionary type\n",
    "    if isinstance(agent.get_actions(), dict):\n",
    "        raise ValueError(\"The action space for Deep Q Learning cannot be of type Dict.\")\n",
    "\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        environment.clock.tick(fps)\n",
    "        state = environment.reset(attributes)\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps_per_episode:\n",
    "            step += 1\n",
    "            # Get list of possible actions from agent\n",
    "            possible_actions = agent.get_actions()\n",
    "            # Convert state to tensor for feeding into the network\n",
    "            state_tensor = th.tensor(state, dtype=th.float32).to(q_network.device)\n",
    "            state_tensor = th.unsqueeze(state_tensor, 0)\n",
    "            # Feed the state into the q_network to get Q-values for each action\n",
    "            with th.no_grad():  # Disable gradient computation\n",
    "                q_values = q_network(state_tensor)\n",
    "            # Choose action with highest Q-value\n",
    "            _, action_index = th.max(q_values, dim=1)\n",
    "            action = possible_actions[action_index.item()]\n",
    "            # Take action in the environment\n",
    "            next_state, reward, done = environment.step(action, 0, attributes)\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "            environment.render_env()\n",
    "\n",
    "## Demo Function Call:\n",
    "demo_model (\n",
    "    agent=snake_env.snake_agent,\n",
    "    environment=snake_env,\n",
    "    q_network=q_net,\n",
    "    num_episodes=100,\n",
    "    max_steps_per_episode=100,\n",
    "    attributes=\"game_state\",\n",
    "    fps=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoderSchoolAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How do We use this in Our Case?\n",
    "\"\"\"\n",
    "Here is the Basic template for Building a Neural Network for our Snake!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "from CoderSchoolAI.Neural.ActorCritic.ActorCriticNetwork import *\n",
    "import torch as th\n",
    "\n",
    "snake_env = SnakeEnv(width=16, height=16)\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False,)\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "# out_block = OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()),)\n",
    "\n",
    "ppo_net = Net(name='test_ppo_net_with_game_state')\n",
    "ppo_net.add_block(input_block)\n",
    "ppo_net.add_block(conv_block)\n",
    "ppo_net.compile()\n",
    "\n",
    "ActorCritic(snake_env.get_observation_space(), snake_env.get_action_space(),ppo_net, net_arch=[32, dict(vf=[32, 32], pi=[32, 32])],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

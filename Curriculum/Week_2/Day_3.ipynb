{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks for the Snake Game\n",
    "\n",
    "This tutorial aims to cover the essentials of building a neural network to play the snake game. The main points are: understanding neural networks, understanding the data, and using the CoderSchoolAI.Neural library.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Neural Networks are a set of algorithms modeled after the human brain. They are designed to recognize patterns, which makes them useful in machine learning and artificial intelligence applications.\n",
    "\n",
    "In the context of our Snake game, we'll be building a neural network to make decisions based on the game's current state. Given the current state, our network will be responsible for deciding which move (up, down, left, right) will maximize the chances of winning the game.\n",
    "\n",
    "## Understanding the Data\n",
    "\n",
    "For the Snake game, our data comes in the form of the game state, actions, and reward:\n",
    "\n",
    "1. **State**: The state of the game is like a snapshot of what the game looks like right now. In Snake, the state might be the position of the snake and the position of the apple.\n",
    "\n",
    "2. **Actions**: Actions are the things you can do in the game. In Snake, the actions would be moving up, down, left, or right.\n",
    "\n",
    "3. **Reward**: The reward is what the game gets for performing an action. In Snake, the reward might be the points received for eating an apple or a penalty for crashing into a wall or the snake itself.\n",
    "\n",
    "## CoderSchoolAI.Neural Library\n",
    "\n",
    "This library provides tools to easily build and manipulate deep neural networks. We will be using the Net class to build our neural network.\n",
    "\n",
    "Here is a brief overview of the classes and methods provided by the library:\n",
    "\n",
    "### Net Class\n",
    "\n",
    "This is the main class that will be used to build the network structure. It has the following methods:\n",
    "\n",
    "- `__init__(self, network_name:str= \"basic_net\", is_dict_network=False, device: th.device = th.device('cpu'))`: This initializes the Net. It can be named for organization, declared as a dictionary network, and assigned a specific device for computation.\n",
    "\n",
    "- `add_block(self, block: Block)`: Adds a Block connection to the network. The type of Block is based on the input.\n",
    "\n",
    "- `compile(self)`: Once all blocks have been added, this function will compile the network and prepare it for training.\n",
    "\n",
    "- `forward(self, x: th.Tensor)`: This function is called during training to pass the input data through the network.\n",
    "\n",
    "- `copy(self)`: This function copies the current network, useful for creating multiple instances of the same network.\n",
    "\n",
    "### InputBlock Class\n",
    "\n",
    "The InputBlock is the first layer of the network, it takes in the input data.\n",
    "\n",
    "- `__init__(self, in_attribute:Union[Attribute, Dict[str, Attribute]], is_module_dict:bool=False, device: th.device = th.device('cpu'))`: Initializes the InputBlock with attributes, declares if it is a module dictionary, and assigns the computational device.\n",
    "\n",
    "- `forward(self, x) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "### LinearBlock Class\n",
    "\n",
    "The LinearBlock is a fully connected layer of the network (MLP).\n",
    "\n",
    "- `__init__(self, input_size: int, output_size: int, num_hidden_layers: int = 3, hidden_size: Union[int, List[int]] = 128, activation: Optional[Callable] = None, device: th.device = th.device('cpu'))`: Initializes the LinearBlock with the input size, output size, number of hidden layers, size of hidden layers, an optional activation function, and the device for computation.\n",
    "\n",
    "- `forward(self, x: th.Tensor) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "### ConvBlock Class\n",
    "\n",
    "The ConvBlock class is for building convolutional neural network (CNN) architectures.\n",
    "\n",
    "Below is an explanation of the main methods within this class:\n",
    "\n",
    "- `__init__(self, input_shape: int, num_channels: int, depth: int = 3, disable_max_pool: bool = False, desired_output_size: int = None, activation: Optional[Callable] = None, device: th.device = th.device('cpu'))`: Initializes the ConvBlock with various parameters including input shape, number of channels, depth (i.e. the number of convolutional layers), and optional parameters such as max pooling disablement, desired output size, and activation function. The computational device can also be set.\n",
    "\n",
    "- `forward(self, x) -> th.Tensor`: This function is called during training to pass the input data through the block. It first checks the input shape to ensure it matches the expected input shape for the ConvBlock, and raises an error if the shape doesn't match. It then passes the input through the block's module (which is a sequence of convolutional, activation and pooling layers) to generate the output. If the block has forward connections, it calls the forward method on the next block in the sequence.\n",
    "\n",
    "### OutputBlock Class\n",
    "\n",
    "The OutputBlock is the final layer of the network which outputs the results.\n",
    "\n",
    "- `__init__(self, input_size, num_classes, device: th.device = th.device('cpu'))`: Initializes the OutputBlock some specified input size to output some number of classes.\n",
    "\n",
    "- `forward(self, x: th.Tensor) -> th.Tensor`: This function is called during training to pass the input data through the block.\n",
    "\n",
    "## Putting it all Together\n",
    "\n",
    "To put all of this together for our Snake game, we will need to create a neural network that can take the game's current state and output a decision for the next action. \n",
    "\n",
    "The basic steps are as follows:\n",
    "\n",
    "1. **Create InputBlock**: We'll need an InputBlock to take in our game state. The state can be an array containing information about the snake's position, the apple's position, and any other relevant game information.\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "```\n",
    "\n",
    "2. **Create Hidden Layers (LinearBlocks)**: These layers will do the computation based on our input. The exact number and size of these layers can vary depending on the complexity of the game and the sophistication of the strategy you want to develop.\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "    lin_block = LinearBlock(input_size=len(snake_env.snake_agent.get_q_table_state()), num_hidden_layers=3, hidden_size=128, dropout=0.05)\n",
    "```\n",
    "\n",
    "3. **Create Image Filter Blocks (ConvBlocks)**: This is a basic architecture commonly used for any image processing tasks. Our SnakeEnv has a GameState which just so happends to be an Image!\n",
    "\n",
    "```python\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "    conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=3)\n",
    "```\n",
    "\n",
    "3. **Create OutputBlock**: Finally, we need an OutputBlock that will output the decision for the next move. This will usually be a single value indicating the direction to move (up, down, left, right).\n",
    "\n",
    "```python\n",
    "    # Output Example with ConvBlocks!\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "    conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=3)\n",
    "    OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "```\n",
    "\n",
    "```python\n",
    "    # Output Example with LinearBlocks!\n",
    "    snake_env = SnakeEnv(width=16, height=16)\n",
    "    input_block = InputBlock(in_attribute=snake_env.snake_agent.get_q_table_state(), is_module_dict=False)\n",
    "    lin_block = LinearBlock(input_size=len(snake_env.snake_agent.get_q_table_state()), num_hidden_layers=3, hidden_size=128, dropout=0.05)\n",
    "    OutputBlock(input_size=lin_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "```\n",
    "\n",
    "4. **Compile the Network**: Once all blocks have been added, compile the network with `compile()` method of the Net class.\n",
    "\n",
    "```python\n",
    "    # Build the Network from Blocks\n",
    "    net = Net()\n",
    "    net.add_block(input_block)\n",
    "    net.add_block(conv_block)\n",
    "    net.add_block(out_block)\n",
    "\n",
    "    # Compile the Network!\n",
    "    net.compile()\n",
    "```\n",
    "\n",
    "\n",
    "5. **Train the Network**: After compiling the network, we'll need to train it. This involves playing the game many times and adjusting the weights of the network based on the outcomes. This process is usually facilitated by reinforcement learning algorithms.\n",
    "\n",
    "\n",
    "    `Think about training a computer to play a video game. It's a lot like teaching a dog a new trick: we want the computer to learn how to do something really well.`\n",
    "\n",
    "- **The Game**: `In the game, our computer (which we'll call the 'agent') is going to play   many rounds (or 'episodes') of the game. In each round, it will try different things to see what works.`\n",
    "\n",
    "- **Deep Q-Learning**:` We're using a special training method called 'Deep Q-Learning.'     This is a set of rules that tells the agent how to play and learn from the game. It's kind  of like the rule book for the agent.`\n",
    "\n",
    "- **Epsilon-Greedy Strategy**: `To start, the agent mostly guesses what to do (we call  these 'actions'). But, over time, it starts to learn which actions work best. It still   guesses sometimes, though, because that could lead to even better actions. We call this the 'Epsilon-Greedy Strategy.' `\n",
    "\n",
    "- **Memory**: `Just like how you remember your best games, the agent also stores its    experiences (the game state, its actions, and rewards) in its memory. This helps it learn  over time.`\n",
    "\n",
    "- **Learning**:` After each episode, the agent takes a look at its memory and learns from   it. It then adjusts its playing strategy based on what it learned.`\n",
    "\n",
    "- **Playing the Game**: `The agent keeps playing the game, adjusting its strategy, and  filling its memory. It does this for a lot of episodes, and with each episode, it should   get a little better!`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a great example, and indeed shows the use of the `CoderSchoolAI.Neural` library in a concise manner. I'll extend this example with more comments for additional clarity:\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary modules\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th\n",
    "\n",
    "# Initialize the game environment with a 16x16 grid\n",
    "snake_env = SnakeEnv(width=8, height=8)\n",
    "\n",
    "# Define the InputBlock. \n",
    "# We use the game state attribute from the environment as the input to our network.\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False)\n",
    "\n",
    "# Define the ConvBlock which acts as a convolutional layer for processing the game state.\n",
    "# The depth of 3 represents the number of convolutional layers in this block.\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape, num_channels=1, depth=3)\n",
    "\n",
    "# Define the OutputBlock that will decide the next action to take based on the current game state.\n",
    "# The num_classes corresponds to the number of possible actions the snake can take (up, down, left, right).\n",
    "# out_block = OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()))\n",
    "\n",
    "# Initialize the network and add the blocks\n",
    "net = Net()\n",
    "net.add_block(input_block)\n",
    "net.add_block(conv_block)\n",
    "# net.add_block(out_block)\n",
    "\n",
    "# Compile the network\n",
    "net.compile()\n",
    "\n",
    "# Test the network\n",
    "# We get a sample game state and feed it through the network.\n",
    "input_sample = snake_env.get_attribute(\"game_state\").sample()\n",
    "net.eval()\n",
    "output_test = net(input_sample)\n",
    "\n",
    "# Test network copying\n",
    "# We create a copy of the network and test it with the same input sample.\n",
    "# This is to verify that the copying operation works correctly.\n",
    "copy_of_net = net.copy()\n",
    "copy_of_net.eval()\n",
    "output_copy_test = copy_of_net(input_sample)\n",
    "# net.compare_networks(copy_of_net)\n",
    "\n",
    "th.testing.assert_close(output_test, output_copy_test, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "The performance of the neural network would depend on the exact architecture (number and type of blocks), as well as the training process, which has not been covered in this code snippet. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoderSchoolAI v0.0.9: \n",
      "CoderSchoolAI: A Python Module designed for teaching Modern Day AI to Kids.\n",
      "\n",
      "This package includes a range of educational tools and templates designed to simplify complex concepts and offer improved learning opportunities. \n",
      "It enables the exploration of foundational principles of problem-solving through Artificial Intelligence, providing structured guidance for the exploration and \n",
      "implementation of theoretical concepts.\n",
      "\n",
      "Key Features:\n",
      " - Learning Curriculum: Our module makes learning programming engaging and fun, turning complex ideas into digestible chunks.\n",
      " \n",
      " - Educational Tools and Templates: We provide tools and templates to simplify complex concepts and enhance learning opportunities.\n",
      " \n",
      " - Exploration of Foundational Concepts: Our module enables the exploration of foundational principles of problem-solving through Artificial Intelligence.\n",
      " - Structural Guidance: We offer structured guidance for exploring and implementing theoretical concepts in a hands-on way.\n",
      " \n",
      " - Simplified Machine Learning Principles: Our module breaks down complex machine learning  principles to facilitate understanding and utilization of these fundamental mechanisms.\n",
      " \n",
      " - Neural Networks to Neural Blocks: Our module implements a simpler concept called neural blocks. A Neural Block is an auto-generated neural network Sequential that can be trained to learn from data. \n",
      " \n",
      " - Training Agents/Bots: Users can train agents or bots to play games, leveraging principles of data engineering and neural networks.\n",
      " \n",
      "Algorithms Available:\n",
      " - Deep-Q w/ Target Network: Deep-Q is a deep reinforcement learning algorithm that uses a target network\n",
      " \n",
      "The goal of this module is to make Agent AI and machine learning more accessible and enjoyable to learn.\n",
      "\n",
      "Future Enhancements:\n",
      " - Adding {Join/Split/Add} support for Creating Network Chains.\n",
      " - Adding Support for Neural Block Graph Architectures.\n",
      " - Support for multiple agents/bots in a single game.\n",
      " - PPO, DDPG, SAC, TD3, PPO2, DDPG2, SAC2, TRPO\n",
      "\n",
      "Known Issues: \n",
      "\n",
      "Development Environment: \n",
      " - Python 3.6+\n",
      "\n",
      "Contribution Guidelines:\n",
      " - Pull from GitHub: https://github.com/theCoderSchoolTampa/CoderSchoolAI/CoderSchoolAI/blob/master/\n",
      " \n",
      " - Make Desired Changes to source code\n",
      " \n",
      " - Document Changelog: \n",
      "        Format [Name] - [Date] - [PR #]:\n",
      "        - [Change]: \n",
      "            [Description]\n",
      "        - [Change]:\n",
      "            [Description]\n",
      "            \n",
      " - Edit README.md Documentation if Applicable\n",
      " \n",
      " - Create a Pull Request: https://github.com/theCoderSchoolTampa/CoderSchoolAI/CoderSchoolAI/pulls/\n",
      " \n",
      " - Grab a Cup o' Joe, and know that I appreciate you for contributing work to this project :D\n",
      "\n",
      "\n",
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Network...\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.InputBlock.InputBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.ConvBlock.ConvBlock>,\n",
      "  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (8): ReLU()\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (11): ReLU()\n",
      "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): Flatten(start_dim=1, end_dim=-1)\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.FlattenBlock.FlattenBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.LinearBlock.LinearBlock>,\n",
      "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.05, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.05, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.05, inplace=False)\n",
      "  (9): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.05, inplace=False)\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.OutputBlock.OutputBlock>,\n",
      "  (0): Linear(in_features=16, out_features=5, bias=True)\n",
      "  (1): Identity()\n",
      "\n",
      "Compiling Network...\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.InputBlock.InputBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.ConvBlock.ConvBlock>,\n",
      "  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (8): ReLU()\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (11): ReLU()\n",
      "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): Flatten(start_dim=1, end_dim=-1)\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.FlattenBlock.FlattenBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.LinearBlock.LinearBlock>,\n",
      "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.05, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.05, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.05, inplace=False)\n",
      "  (9): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.05, inplace=False)\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.OutputBlock.OutputBlock>,\n",
      "  (0): Linear(in_features=16, out_features=5, bias=True)\n",
      "  (1): Identity()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### How do We use this in Our Case?\n",
    "\"\"\"\n",
    "Here is the Basic template for Building a Neural Network for our Snake!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.FrozenLakeEnvironment import *\n",
    "\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "from CoderSchoolAI.Neural.ActorCritic.ActorCriticNetwork import *\n",
    "import torch as th\n",
    "\n",
    "# To use attribute we must make a copy to avoid issues saving the data\n",
    "# TODO: fix this copying issue\n",
    "\n",
    "snake_env = SnakeEnv(width=8, height=8)\n",
    "# frozen_lake_env = FrozenLakeEnv()\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\").copy(), is_module_dict=False,)\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "flatten_size = np.prod(snake_env.get_attribute(\"game_state\").shape)\n",
    "flat_block = FlattenBlock(conv_block.output_size)\n",
    "lin_block = LinearBlock(flat_block.output_size, 16, num_hidden_layers=3, dropout=0.05, hidden_size=[128, 128, 32])\n",
    "\n",
    "out_block = OutputBlock(input_size=lin_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()),)\n",
    "\n",
    "q_net = Net(name='test_q_net_with_game_state')\n",
    "q_net.add_block(input_block)\n",
    "q_net.add_block(conv_block)\n",
    "q_net.add_block(flat_block)\n",
    "q_net.add_block(lin_block)\n",
    "q_net.add_block(out_block)\n",
    "q_net.compile()\n",
    "\n",
    "copy_net = q_net.copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`deep_q_learning` function:\n",
    "\n",
    "- **agent**: This is our player or the character in the game. \n",
    "\n",
    "- **environment**: This is the game world or the level where our character is playing.\n",
    "\n",
    "- **q_network**: Think of this as the brain of our character. It's what helps the character make decisions based on what it has learned.\n",
    "\n",
    "- **target_q_network**: This is a copy of the character's brain, but we only update it from time to time. We use it to help the character make more stable decisions.\n",
    "\n",
    "- **buffer**: This is the character's memory. It stores what the character has seen, what it did, and what happened.\n",
    "\n",
    "- **num_episodes**: This is how many rounds of the game the character will play. More rounds mean more practice!\n",
    "\n",
    "- **max_steps_per_episode**: This is the maximum number of moves the character can make in one round. If a round takes too long, we stop it.\n",
    "\n",
    "- **gamma**: This is a measure of how much our character cares about future rewards compared to immediate ones. A high value makes the character care more about long-term results.\n",
    "\n",
    "- **update_target_every**: This tells us how often we update the copy of the character's brain. For example, if it's set to 10, we update it after every 10 rounds.\n",
    "\n",
    "- **batch_size**: This is how many memories we use when teaching the character from its past experiences.\n",
    "\n",
    "- **epsilon**: This is how often the character makes random moves. At first, we want the character to explore a lot and try different things, so we set this high.\n",
    "\n",
    "- **epsilon_decay**: This is how quickly we reduce the amount of randomness in the character's actions. As the character learns more, we want it to rely less on random moves.\n",
    "\n",
    "- **stop_epsilon**: This is the lowest amount of randomness we allow in the character's actions. We always want it to try something new from time to time!\n",
    "\n",
    "- **alpha**: This is how quickly the character learns. A high value means the character learns quickly but might miss some details. A low value means slower learning, but more attention to details.\n",
    "\n",
    "- **attributes**: These are extra details or features that the character can use to make decisions. For example, it could be the character's health or number of lives in the game.\n",
    "\n",
    "- **optimizer**: This is a tool that helps the character's brain learn from its mistakes and get better.\n",
    "\n",
    "- **fps**: This is how fast the game runs. A higher number means the game runs faster.\n",
    "\n",
    "Playing around with these parameters changes how the character learns and plays the game. So go ahead and tweak them to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2, Avg Reward: -6.5986842376526464, Epsilon: 0.49985001499950005\n",
      "Episode: 3, Avg Reward: 2.367143796475936, Epsilon: 0.4995501799580064\n",
      "Episode: 4, Avg Reward: -1.7322323589076611, Epsilon: 0.4993503898570359\n",
      "Episode: 5, Avg Reward: -1.8061313726086914, Epsilon: 0.4993004548180502\n",
      "Episode: 6, Avg Reward: -0.7954624872093603, Epsilon: 0.4990009494302423\n",
      "Episode: 7, Avg Reward: -1.3900815923803544, Epsilon: 0.4988512641149427\n",
      "Episode: 8, Avg Reward: -2.054329488807275, Epsilon: 0.49875149885063236\n",
      "Episode: 9, Avg Reward: 0.8652871007766461, Epsilon: 0.4986018883630234\n",
      "Episode: 10, Avg Reward: -2.196747409450275, Epsilon: 0.4985520281741871\n",
      "Episode: 11, Avg Reward: 11.793577530163232, Epsilon: 0.4979042992655925\n",
      "Episode: 12, Avg Reward: -1.2828890951158067, Epsilon: 0.4978047233847824\n",
      "Episode: 13, Avg Reward: -0.6747628229587972, Epsilon: 0.4976056313617207\n",
      "Episode: 14, Avg Reward: 0.7462970623752176, Epsilon: 0.49740661896352356\n",
      "Episode: 15, Avg Reward: 0.17057183625223615, Epsilon: 0.4969591320310634\n",
      "Episode: 16, Avg Reward: -2.0647172992352107, Epsilon: 0.4968597451742485\n",
      "Episode: 17, Avg Reward: -2.2243225052254183, Epsilon: 0.4968100591997311\n",
      "Episode: 18, Avg Reward: -0.6712165644120205, Epsilon: 0.4966610310857762\n",
      "Episode: 19, Avg Reward: -1.5399538014276626, Epsilon: 0.4965120476757848\n",
      "Episode: 20, Avg Reward: -0.15780566048436118, Epsilon: 0.49626384129818685\n",
      "Episode: 21, Avg Reward: -2.3186145306183255, Epsilon: 0.49621421491405704\n",
      "Episode: 22, Avg Reward: 9.33405133452564, Epsilon: 0.4955199663875407\n",
      "Episode: 23, Avg Reward: -1.3298521580663372, Epsilon: 0.49547041439090195\n",
      "Episode: 24, Avg Reward: -1.9751295407593246, Epsilon: 0.4954208673494629\n",
      "Episode: 25, Avg Reward: -1.7320461913190321, Epsilon: 0.4952227287257935\n",
      "Episode: 26, Avg Reward: 1.6033612491776457, Epsilon: 0.4951236891322756\n",
      "Episode: 27, Avg Reward: -1.6364256460505886, Epsilon: 0.49502466934568606\n",
      "Episode: 28, Avg Reward: -0.8318578366068758, Epsilon: 0.49477720650853013\n",
      "Episode: 29, Avg Reward: -0.07221058395842173, Epsilon: 0.49448041439131124\n",
      "Episode: 30, Avg Reward: -2.250503809636436, Epsilon: 0.4944309663498721\n",
      "Episode: 31, Avg Reward: -1.6022027909042018, Epsilon: 0.4942332236272125\n",
      "Episode: 32, Avg Reward: -1.0621333071513273, Epsilon: 0.49403555998977816\n",
      "Episode: 33, Avg Reward: 1.143949953935457, Epsilon: 0.4937392127492383\n",
      "Episode: 34, Avg Reward: 0.12356128992372328, Epsilon: 0.4935417466865165\n",
      "Episode: 35, Avg Reward: -0.8551632516363692, Epsilon: 0.4935417466865165\n",
      "Episode: 36, Avg Reward: 0.0019027178121908683, Epsilon: 0.49319637109033054\n",
      "Episode: 37, Avg Reward: -1.1166954359821628, Epsilon: 0.4931470514532215\n",
      "Episode: 38, Avg Reward: -1.8132920149550193, Epsilon: 0.492999122131704\n",
      "Episode: 39, Avg Reward: -1.4008285567177468, Epsilon: 0.4928512371845452\n",
      "Episode: 40, Avg Reward: -2.2626623801182943, Epsilon: 0.4928019520608268\n",
      "Episode: 41, Avg Reward: -0.97986305947665, Epsilon: 0.4926541262587743\n",
      "Episode: 42, Avg Reward: -1.9817231746436976, Epsilon: 0.4926048608461484\n",
      "Episode: 43, Avg Reward: -0.21366748178800066, Epsilon: 0.49235860767128564\n",
      "Episode: 44, Avg Reward: -0.8597363578653563, Epsilon: 0.49226014087333747\n",
      "Episode: 45, Avg Reward: -1.064713677657237, Epsilon: 0.4921616937677642\n",
      "Episode: 46, Avg Reward: -2.234782624408221, Epsilon: 0.49211247759838744\n",
      "Episode: 47, Avg Reward: 4.234129624995785, Epsilon: 0.4919156621321284\n",
      "Episode: 48, Avg Reward: -2.20277364156796, Epsilon: 0.4918664705659152\n",
      "Episode: 49, Avg Reward: -2.027431791936705, Epsilon: 0.4918172839188586\n",
      "Episode: 50, Avg Reward: -1.7774685400448387, Epsilon: 0.49176810219046674\n",
      "Episode: 51, Avg Reward: -0.7602857520350645, Epsilon: 0.49157142445370966\n",
      "Episode: 52, Avg Reward: -1.2853055142986887, Epsilon: 0.4914239677730247\n",
      "Episode: 53, Avg Reward: -1.1525762489611844, Epsilon: 0.491227427669388\n",
      "Episode: 54, Avg Reward: 2.645189978454752, Epsilon: 0.4909327648870767\n",
      "Episode: 55, Avg Reward: 2.49829947225285, Epsilon: 0.49049110209324154\n",
      "Episode: 56, Avg Reward: -0.9743846733810273, Epsilon: 0.49039300877773395\n",
      "Episode: 57, Avg Reward: -1.8707396706198787, Epsilon: 0.49029493507990846\n",
      "Episode: 58, Avg Reward: -0.7534010412142202, Epsilon: 0.4900988465216115\n",
      "Episode: 59, Avg Reward: 1.1423421376793526, Epsilon: 0.4898048607187243\n",
      "Episode: 60, Avg Reward: 1.2187520322961811, Epsilon: 0.48965793395416474\n",
      "Episode: 61, Avg Reward: 2.2501073052599203, Epsilon: 0.48921741804933716\n",
      "Episode: 62, Avg Reward: 4.184403758978227, Epsilon: 0.4889728582571624\n",
      "Episode: 63, Avg Reward: -2.250282027557828, Epsilon: 0.4889239609713367\n",
      "Episode: 64, Avg Reward: -0.45042869102322336, Epsilon: 0.4887772984502753\n",
      "Episode: 65, Avg Reward: -2.2348386551915578, Epsilon: 0.48872842072043027\n",
      "Episode: 66, Avg Reward: -1.11783774917969, Epsilon: 0.4886306799235704\n",
      "Episode: 67, Avg Reward: -1.479854708598977, Epsilon: 0.48858181685557805\n",
      "Episode: 68, Avg Reward: -0.8310772355794191, Epsilon: 0.4885329586738925\n",
      "Episode: 69, Avg Reward: 5.458385728698008, Epsilon: 0.48819108817764517\n",
      "Episode: 70, Avg Reward: 1.759753165555547, Epsilon: 0.48804464549643634\n",
      "Episode: 71, Avg Reward: -0.7561942008671874, Epsilon: 0.4879470414477835\n",
      "Episode: 72, Avg Reward: 2.0370921326540747, Epsilon: 0.48755682041247533\n",
      "Episode: 73, Avg Reward: 0.4002720133616662, Epsilon: 0.48726435944400054\n",
      "Loss: 1.7145923376083374\n",
      "Episode: 74, Avg Reward: -0.48612867211197264, Epsilon: 0.4870694829341355\n",
      "Episode: 75, Avg Reward: -2.229027778808704, Epsilon: 0.4870207759858421\n",
      "Episode: 76, Avg Reward: 0.8657510452314576, Epsilon: 0.48682599689474626\n",
      "Episode: 77, Avg Reward: -2.1358194801860573, Epsilon: 0.48672863656362725\n",
      "Episode: 78, Avg Reward: -2.2185634295532575, Epsilon: 0.4866799636999709\n",
      "Episode: 79, Avg Reward: 0.8804324465871303, Epsilon: 0.48653397431077317\n",
      "Episode: 80, Avg Reward: -0.14595984822862862, Epsilon: 0.48653397431077317\n",
      "Episode: 81, Avg Reward: -1.6417724458407168, Epsilon: 0.4864366723812507\n",
      "Episode: 82, Avg Reward: -1.542064600619279, Epsilon: 0.48633938991114123\n",
      "Episode: 83, Avg Reward: 4.8383792360826705, Epsilon: 0.4859990544524551\n",
      "Episode: 84, Avg Reward: 0.09122582217189779, Epsilon: 0.4859018595015552\n",
      "Episode: 85, Avg Reward: -2.212030553770265, Epsilon: 0.485853269315605\n",
      "Episode: 86, Avg Reward: 1.2149900295763918, Epsilon: 0.4856589571571316\n",
      "Episode: 87, Avg Reward: -0.8703626102813904, Epsilon: 0.48546472271186353\n",
      "Episode: 88, Avg Reward: 3.5662822822812856, Epsilon: 0.4852220388921255\n",
      "Episode: 89, Avg Reward: 1.9953866191895173, Epsilon: 0.4850764868366338\n",
      "Episode: 90, Avg Reward: 2.343049843883941, Epsilon: 0.4847370351449345\n",
      "Episode: 91, Avg Reward: -1.9123386810416814, Epsilon: 0.48468856144142003\n",
      "Episode: 92, Avg Reward: -0.8708497403180222, Epsilon: 0.4846400925852759\n",
      "Episode: 93, Avg Reward: -0.5820125883315856, Epsilon: 0.4845916285760174\n",
      "Episode: 94, Avg Reward: 0.11411827100876382, Epsilon: 0.4845431694131598\n",
      "Episode: 95, Avg Reward: 7.689977374002341, Epsilon: 0.4841072549555335\n",
      "Episode: 96, Avg Reward: 0.4538221699767204, Epsilon: 0.483816863208967\n",
      "Episode: 97, Avg Reward: -1.8586050916513779, Epsilon: 0.4837684815226461\n",
      "Episode: 98, Avg Reward: -1.6574899931531768, Epsilon: 0.48367173266402647\n",
      "Episode: 99, Avg Reward: -1.0049213749312704, Epsilon: 0.4834782929893302\n",
      "Episode: 100, Avg Reward: -0.6867402080255254, Epsilon: 0.4832849306788982\n",
      "Episode: 101, Avg Reward: 1.7522138950445214, Epsilon: 0.48289843802707516\n",
      "Episode: 102, Avg Reward: -0.02774382676869358, Epsilon: 0.48285014818327243\n",
      "Episode: 103, Avg Reward: 5.090602716376979, Epsilon: 0.4827535829821373\n",
      "Episode: 104, Avg Reward: -2.2506941228184076, Epsilon: 0.48270530762383906\n",
      "Episode: 105, Avg Reward: 0.42680789453989787, Epsilon: 0.4824157568354075\n",
      "Episode: 106, Avg Reward: -0.9717631628173755, Epsilon: 0.48217459719374156\n",
      "Episode: 107, Avg Reward: -0.028546180273406385, Epsilon: 0.4820781670960488\n",
      "Episode: 108, Avg Reward: -0.5031380949727018, Epsilon: 0.4819335581077829\n",
      "Episode: 109, Avg Reward: -0.061792870150937595, Epsilon: 0.4817889924978753\n",
      "Episode: 110, Avg Reward: -1.3709321931878466, Epsilon: 0.48169263951726565\n",
      "Episode: 111, Avg Reward: 4.578407839142647, Epsilon: 0.4815963058062886\n",
      "Episode: 112, Avg Reward: -2.2591614728974365, Epsilon: 0.48154814617570796\n",
      "Episode: 113, Avg Reward: -2.2506449719477515, Epsilon: 0.4814999913610904\n",
      "Episode: 114, Avg Reward: -2.242224066025487, Epsilon: 0.4814518413619543\n",
      "Episode: 115, Avg Reward: -0.4242237200446273, Epsilon: 0.48125928951059427\n",
      "Episode: 116, Avg Reward: -1.6661197624881559, Epsilon: 0.48111492616103857\n",
      "Episode: 117, Avg Reward: -0.27987118746814366, Epsilon: 0.4808263293629594\n",
      "Episode: 118, Avg Reward: -0.0013254109679206039, Epsilon: 0.4806820958884596\n",
      "Episode: 119, Avg Reward: -1.1219995779841925, Epsilon: 0.48048985188910737\n",
      "Episode: 120, Avg Reward: -1.0893907660235957, Epsilon: 0.4803937587236281\n",
      "Episode: 121, Avg Reward: -0.06941046241169291, Epsilon: 0.480297684775821\n",
      "Episode: 122, Avg Reward: 0.3757455706636912, Epsilon: 0.4801055945178507\n",
      "Episode: 123, Avg Reward: -2.2512529343549197, Epsilon: 0.48005758395839887\n",
      "Episode: 124, Avg Reward: 1.7989298167305137, Epsilon: 0.4799615772421831\n",
      "Episode: 125, Avg Reward: 0.6222688709410229, Epsilon: 0.4798655897263504\n",
      "Episode: 126, Avg Reward: -0.7780038634789423, Epsilon: 0.47976962140706103\n",
      "Episode: 127, Avg Reward: 2.563369018358848, Epsilon: 0.47938594001856566\n",
      "Episode: 128, Avg Reward: -2.2566657729438018, Epsilon: 0.4793380014245638\n",
      "Episode: 129, Avg Reward: 1.7749293211596608, Epsilon: 0.4791462949823568\n",
      "Episode: 130, Avg Reward: -2.259098465259135, Epsilon: 0.47909838035285857\n",
      "Episode: 131, Avg Reward: 4.670039330536398, Epsilon: 0.4789067697447039\n",
      "Episode: 132, Avg Reward: 4.113084768059828, Epsilon: 0.47857163555954424\n",
      "Episode: 133, Avg Reward: -0.9509320961459746, Epsilon: 0.4784759260181487\n",
      "Episode: 134, Avg Reward: -2.266517478357814, Epsilon: 0.4784280784255469\n",
      "Episode: 135, Avg Reward: -2.25932223683034, Epsilon: 0.4783802356177044\n",
      "Episode: 136, Avg Reward: -0.06012196882570153, Epsilon: 0.47814109333313554\n",
      "Episode: 137, Avg Reward: 1.280473655956067, Epsilon: 0.47785428038873756\n",
      "Episode: 138, Avg Reward: 0.07490061433913242, Epsilon: 0.4775198827251412\n",
      "Episode: 139, Avg Reward: 1.3259281576347344, Epsilon: 0.47723344241393884\n",
      "Episode: 140, Avg Reward: -0.27677823858187933, Epsilon: 0.47704257766907093\n",
      "Episode: 141, Avg Reward: -0.6443933189081537, Epsilon: 0.4769471739239629\n",
      "Episode: 142, Avg Reward: -2.279390420722982, Epsilon: 0.4768994792065705\n",
      "Loss: 1.7095659971237183\n",
      "Episode: 143, Avg Reward: 0.1240965123754596, Epsilon: 0.4767087480269491\n",
      "Episode: 144, Avg Reward: 0.8140099075029119, Epsilon: 0.47647044131904365\n",
      "Episode: 145, Avg Reward: 4.286360324896304, Epsilon: 0.4762322537406638\n",
      "Episode: 146, Avg Reward: 0.7558980826517061, Epsilon: 0.4759465858137336\n",
      "Episode: 147, Avg Reward: -1.7001773829242421, Epsilon: 0.4758989911551522\n",
      "Episode: 148, Avg Reward: 5.363707350995763, Epsilon: 0.47528069353179053\n",
      "Episode: 149, Avg Reward: -0.6197626009219972, Epsilon: 0.47518564214589115\n",
      "Episode: 150, Avg Reward: 0.9097428914798473, Epsilon: 0.47480562665754716\n",
      "Episode: 151, Avg Reward: 1.2039781506641187, Epsilon: 0.47456827132003326\n",
      "Episode: 152, Avg Reward: 0.18621281672448164, Epsilon: 0.474473362411452\n",
      "Episode: 153, Avg Reward: -1.9974466970202323, Epsilon: 0.47442591507521087\n",
      "Episode: 154, Avg Reward: -0.44631977997680705, Epsilon: 0.474331034636455\n",
      "Episode: 155, Avg Reward: -1.473467515610721, Epsilon: 0.474236173172838\n",
      "Episode: 156, Avg Reward: 3.425180438540602, Epsilon: 0.4737147741340098\n",
      "Episode: 157, Avg Reward: 0.2881561061241116, Epsilon: 0.4733832732556402\n",
      "Episode: 158, Avg Reward: 0.6230046687256281, Epsilon: 0.4731466289526061\n",
      "Episode: 159, Avg Reward: -0.8615946166223112, Epsilon: 0.47309931428971086\n",
      "Episode: 160, Avg Reward: -1.0455498565087253, Epsilon: 0.47300469915784604\n",
      "Episode: 161, Avg Reward: 0.03627090709191938, Epsilon: 0.47281552565657287\n",
      "Episode: 162, Avg Reward: -2.3466638511274738, Epsilon: 0.4727682441040072\n",
      "Episode: 163, Avg Reward: -0.06848152160531118, Epsilon: 0.4725319072540522\n",
      "Episode: 164, Avg Reward: 0.4043856171232223, Epsilon: 0.4723429228411749\n",
      "Episode: 165, Avg Reward: -0.820483076420532, Epsilon: 0.4722484589800359\n",
      "Episode: 166, Avg Reward: -1.9981859318539885, Epsilon: 0.47220123413413795\n",
      "Episode: 167, Avg Reward: 1.340549181349866, Epsilon: 0.47191798421439934\n",
      "Episode: 168, Avg Reward: -1.2126839468924204, Epsilon: 0.4717764229762026\n",
      "Episode: 169, Avg Reward: -2.346341021421427, Epsilon: 0.471729245333905\n",
      "Episode: 170, Avg Reward: -0.7240802426444617, Epsilon: 0.47154058193763926\n",
      "Episode: 171, Avg Reward: 0.43773458645872143, Epsilon: 0.47130485879601347\n",
      "Episode: 172, Avg Reward: 0.7381609780848724, Epsilon: 0.47121060253730285\n",
      "Episode: 173, Avg Reward: 1.0511331267719624, Epsilon: 0.47092794684794737\n",
      "Episode: 174, Avg Reward: -2.088960427646713, Epsilon: 0.47083376596785725\n",
      "Episode: 175, Avg Reward: -1.2502730625747331, Epsilon: 0.4706925299626091\n",
      "Episode: 176, Avg Reward: 5.23651370018462, Epsilon: 0.47055133632392554\n",
      "Episode: 177, Avg Reward: 4.238881528903118, Epsilon: 0.4699399864822028\n",
      "Episode: 178, Avg Reward: -0.889010604747555, Epsilon: 0.46984600318430625\n",
      "Episode: 179, Avg Reward: -1.4975298838250986, Epsilon: 0.4697990185839878\n",
      "Episode: 180, Avg Reward: -0.27504554845064977, Epsilon: 0.4696111271626162\n",
      "Episode: 181, Avg Reward: -1.2522858596024957, Epsilon: 0.4695641660499\n",
      "Episode: 182, Avg Reward: -2.013850675898733, Epsilon: 0.4694702579123317\n",
      "Episode: 183, Avg Reward: 0.19959650935083717, Epsilon: 0.46918864616873435\n",
      "Episode: 184, Avg Reward: -2.0819920830122074, Epsilon: 0.46909481313138707\n",
      "Episode: 185, Avg Reward: 0.5478968015221795, Epsilon: 0.46871966860116365\n",
      "Episode: 186, Avg Reward: -1.3203249754823783, Epsilon: 0.4685790667617047\n",
      "Episode: 187, Avg Reward: 5.923517871713798, Epsilon: 0.4682043346841968\n",
      "Episode: 188, Avg Reward: 3.4308872557589805, Epsilon: 0.4680638874294534\n",
      "Episode: 189, Avg Reward: -0.48445890963563665, Epsilon: 0.4679702793326064\n",
      "Episode: 190, Avg Reward: 2.60201888012327, Epsilon: 0.4677831192972183\n",
      "Episode: 191, Avg Reward: 0.6477752219399466, Epsilon: 0.46768956735119\n",
      "Episode: 192, Avg Reward: 0.5576643018101226, Epsilon: 0.4675025195837529\n",
      "Episode: 193, Avg Reward: -0.21434009366357687, Epsilon: 0.4674090237548613\n",
      "Episode: 194, Avg Reward: -1.1944903474557265, Epsilon: 0.4673155466242006\n",
      "Episode: 195, Avg Reward: 4.182867819937677, Epsilon: 0.4666150637730236\n",
      "Episode: 196, Avg Reward: -2.3517487485576596, Epsilon: 0.4665684022666463\n",
      "Episode: 197, Avg Reward: -1.571755209926626, Epsilon: 0.466475093251877\n",
      "Episode: 198, Avg Reward: 2.156623823876922, Epsilon: 0.46628853120121605\n",
      "Episode: 199, Avg Reward: -1.2573337944555498, Epsilon: 0.46600882801645\n",
      "Episode: 200, Avg Reward: -1.4402583065114642, Epsilon: 0.465915630910935\n",
      "Episode: 201, Avg Reward: -0.65740032656152, Epsilon: 0.46577587019866473\n",
      "Episode: 202, Avg Reward: -1.7909258300319597, Epsilon: 0.46549647453361126\n",
      "Episode: 203, Avg Reward: 1.6033885737572269, Epsilon: 0.4650311874766414\n",
      "Episode: 204, Avg Reward: -0.7802070643800365, Epsilon: 0.4648452029016619\n",
      "Loss: 1.6950180530548096\n",
      "Episode: 205, Avg Reward: 7.980373012456542, Epsilon: 0.4641484229733315\n",
      "Episode: 206, Avg Reward: -0.5944020545264266, Epsilon: 0.46410200813103414\n",
      "Episode: 207, Avg Reward: -1.589919707642761, Epsilon: 0.4639163951720459\n",
      "Episode: 208, Avg Reward: -1.7355124347370976, Epsilon: 0.46377723417052225\n",
      "Episode: 209, Avg Reward: 0.06635880710454911, Epsilon: 0.46359175110163303\n",
      "Episode: 210, Avg Reward: -1.9972378782824023, Epsilon: 0.4634990373873302\n",
      "Episode: 211, Avg Reward: -0.2562527196605453, Epsilon: 0.4634526874835915\n",
      "Episode: 212, Avg Reward: -2.3440933357871723, Epsilon: 0.4634063422148431\n",
      "Episode: 213, Avg Reward: -0.19084887944994566, Epsilon: 0.46312836791119816\n",
      "Episode: 214, Avg Reward: -2.340586801334983, Epsilon: 0.46308205507440703\n",
      "Episode: 215, Avg Reward: -0.8738562321772343, Epsilon: 0.4629431443498833\n",
      "Episode: 216, Avg Reward: -0.2624917972008327, Epsilon: 0.4629431443498833\n",
      "Episode: 217, Avg Reward: -2.335040628331506, Epsilon: 0.46289685003544834\n",
      "Episode: 218, Avg Reward: 8.117385350139639, Epsilon: 0.46252666214061905\n",
      "Episode: 219, Avg Reward: -2.1531273167684706, Epsilon: 0.462480409474405\n",
      "Episode: 220, Avg Reward: -2.3316493309873354, Epsilon: 0.46243416143345756\n",
      "Episode: 221, Avg Reward: -2.3270982540508975, Epsilon: 0.4623879180173142\n",
      "Episode: 222, Avg Reward: -1.9864378331036037, Epsilon: 0.4623416792255125\n",
      "Episode: 223, Avg Reward: 1.9581910883467466, Epsilon: 0.4622029905915329\n",
      "Episode: 224, Avg Reward: -1.2713775067856805, Epsilon: 0.46211055461544454\n",
      "Episode: 225, Avg Reward: -1.9666547052752699, Epsilon: 0.462064343559983\n",
      "Episode: 226, Avg Reward: -0.4240935120014844, Epsilon: 0.4619257381183833\n",
      "Episode: 227, Avg Reward: -1.5698487970281128, Epsilon: 0.46187954554457145\n",
      "Episode: 228, Avg Reward: -2.3043881243644115, Epsilon: 0.461833357590017\n",
      "Episode: 229, Avg Reward: -0.7382302101468872, Epsilon: 0.4616948214372789\n",
      "Episode: 230, Avg Reward: 2.0856845201624528, Epsilon: 0.46123333432311764\n",
      "Episode: 231, Avg Reward: -1.9514096453906524, Epsilon: 0.46118721098968535\n",
      "Episode: 232, Avg Reward: 2.1049274775110325, Epsilon: 0.46077230848845674\n",
      "Episode: 233, Avg Reward: -0.41172081386593407, Epsilon: 0.46068015863448214\n",
      "Episode: 234, Avg Reward: -0.8406718415472014, Epsilon: 0.4604498646185742\n",
      "Episode: 235, Avg Reward: -0.5106021999561643, Epsilon: 0.4603577792501491\n",
      "Episode: 236, Avg Reward: -0.14868315116202835, Epsilon: 0.46017366375807445\n",
      "Episode: 237, Avg Reward: 2.297331484860769, Epsilon: 0.46008163362705945\n",
      "Episode: 238, Avg Reward: -2.307713477810341, Epsilon: 0.46003562546369675\n",
      "Episode: 239, Avg Reward: -1.5795533228905065, Epsilon: 0.4599436229389603\n",
      "Episode: 240, Avg Reward: 1.3841514495069407, Epsilon: 0.4596677257475422\n",
      "Episode: 241, Avg Reward: -0.39983763873119327, Epsilon: 0.45948388623546815\n",
      "Episode: 242, Avg Reward: -2.066062909706059, Epsilon: 0.4594379378468446\n",
      "Episode: 243, Avg Reward: -1.888031953930327, Epsilon: 0.4593460548536546\n",
      "Episode: 244, Avg Reward: 1.766911935544659, Epsilon: 0.45925419023614444\n",
      "Episode: 245, Avg Reward: -2.293069777302309, Epsilon: 0.45920826481712085\n",
      "Episode: 246, Avg Reward: 2.6281719700841024, Epsilon: 0.4591164277562401\n",
      "Episode: 247, Avg Reward: -1.5250518563560047, Epsilon: 0.458978706600947\n",
      "Episode: 248, Avg Reward: -0.4979388199491541, Epsilon: 0.4588410267578689\n",
      "Episode: 249, Avg Reward: -1.8338406063943191, Epsilon: 0.4587951426551931\n",
      "Episode: 250, Avg Reward: -1.7598299296480941, Epsilon: 0.45865751787579206\n",
      "Episode: 251, Avg Reward: -0.8761500403484896, Epsilon: 0.4586116521240045\n",
      "Episode: 252, Avg Reward: -1.3495282861847953, Epsilon: 0.45851993437969624\n",
      "Episode: 253, Avg Reward: -1.08838974790902, Epsilon: 0.45847408238625825\n",
      "Episode: 254, Avg Reward: 8.995622564189993, Epsilon: 0.45801581456220186\n",
      "Episode: 255, Avg Reward: 0.41595703532204453, Epsilon: 0.45778685245192235\n",
      "Episode: 256, Avg Reward: -0.5621244214343808, Epsilon: 0.4576952996593005\n",
      "Episode: 257, Avg Reward: -1.0846619778628785, Epsilon: 0.4576037651763216\n",
      "Episode: 258, Avg Reward: 0.9295863860434155, Epsilon: 0.4573750090495342\n",
      "Episode: 259, Avg Reward: -1.6403168626595575, Epsilon: 0.45728353862147436\n",
      "Episode: 260, Avg Reward: -0.14469417074672775, Epsilon: 0.4571920864865855\n",
      "Episode: 261, Avg Reward: -1.361179132987976, Epsilon: 0.45714636727793684\n",
      "Episode: 262, Avg Reward: -1.5748217951382928, Epsilon: 0.4570549425759449\n",
      "Episode: 263, Avg Reward: -2.2718000290082556, Epsilon: 0.45700923708168734\n",
      "Episode: 264, Avg Reward: -0.7052495389922373, Epsilon: 0.45687214802038295\n",
      "Episode: 265, Avg Reward: -1.328745985264407, Epsilon: 0.4566894265716762\n",
      "Episode: 266, Avg Reward: 0.23725358150241016, Epsilon: 0.45641548141001415\n",
      "Episode: 267, Avg Reward: 2.390678844459183, Epsilon: 0.45632420287788694\n",
      "Episode: 268, Avg Reward: -1.9651190136023633, Epsilon: 0.4562329426005534\n",
      "Episode: 269, Avg Reward: -0.6830008773125156, Epsilon: 0.4560504767956649\n",
      "Episode: 270, Avg Reward: -0.6755779567830513, Epsilon: 0.45595927126081054\n",
      "Episode: 271, Avg Reward: -1.3745357477205953, Epsilon: 0.4559136753336845\n",
      "Episode: 272, Avg Reward: -1.390132959921611, Epsilon: 0.4559136753336845\n",
      "Episode: 273, Avg Reward: 2.691012492565747, Epsilon: 0.4558224971577545\n",
      "Episode: 274, Avg Reward: -2.0108226163143055, Epsilon: 0.45577691490803873\n",
      "Episode: 275, Avg Reward: 0.42596659083463617, Epsilon: 0.45573133721654796\n",
      "Episode: 276, Avg Reward: -1.2631338040116264, Epsilon: 0.4555946314868674\n",
      "Episode: 277, Avg Reward: 3.1451374182042002, Epsilon: 0.455275810903755\n",
      "Episode: 278, Avg Reward: 0.3278247757634789, Epsilon: 0.4550482185213317\n",
      "Episode: 279, Avg Reward: -1.8692365921283012, Epsilon: 0.4549117177067668\n",
      "Episode: 280, Avg Reward: 3.625379345529942, Epsilon: 0.45477525783835143\n",
      "Episode: 281, Avg Reward: -0.42098735673470733, Epsilon: 0.45468430733453635\n",
      "Episode: 282, Avg Reward: -0.001511351827437224, Epsilon: 0.4546388389038029\n",
      "Episode: 283, Avg Reward: 2.374527617197666, Epsilon: 0.45432068717481566\n",
      "Loss: 1.6966259479522705\n",
      "Episode: 284, Avg Reward: 12.001127346953044, Epsilon: 0.4538211842203534\n",
      "Episode: 285, Avg Reward: -2.2731288575119617, Epsilon: 0.45377580210193136\n",
      "Episode: 286, Avg Reward: -2.0791279247082928, Epsilon: 0.4537304245217212\n",
      "Episode: 287, Avg Reward: -0.664576944494607, Epsilon: 0.453685051479269\n",
      "Episode: 288, Avg Reward: -1.8735753938449091, Epsilon: 0.4536396829741211\n",
      "Episode: 289, Avg Reward: -2.0262535099498598, Epsilon: 0.4535943190058237\n",
      "Episode: 290, Avg Reward: -0.6502470147902732, Epsilon: 0.45350360467796574\n",
      "Episode: 291, Avg Reward: -1.3046774214186438, Epsilon: 0.453367567201217\n",
      "Episode: 292, Avg Reward: -2.2605109925411875, Epsilon: 0.4533222304444969\n",
      "Episode: 293, Avg Reward: -0.1807559564119865, Epsilon: 0.45305030509549904\n",
      "Episode: 294, Avg Reward: -1.3391225381405616, Epsilon: 0.452869112154667\n",
      "Episode: 295, Avg Reward: 0.30257167396246487, Epsilon: 0.4525974586086844\n",
      "Episode: 296, Avg Reward: -1.7089688425209533, Epsilon: 0.45246169294857297\n",
      "Episode: 297, Avg Reward: -2.2612544294114483, Epsilon: 0.4524164467792781\n",
      "Episode: 298, Avg Reward: 2.5018088482570278, Epsilon: 0.45232596801408675\n",
      "Episode: 299, Avg Reward: -1.787063722384894, Epsilon: 0.45219028379300924\n",
      "Episode: 300, Avg Reward: -1.8040936410589365, Epsilon: 0.45214506476462996\n",
      "Episode: 301, Avg Reward: 1.5392982964928277, Epsilon: 0.4520094348091004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCoderSchoolAI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEnvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicReplayBuffer, DictReplayBuffer\n\u001b[0;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdeep_q_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnake_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnake_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnake_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_q_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_target_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBasicReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_norm_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_normalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgame_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:198\u001b[0m, in \u001b[0;36mdeep_q_learning\u001b[1;34m(agent, environment, q_network, target_q_network, buffer, num_episodes, max_steps_per_episode, gamma, update_target_every, batch_size, epsilon, epsilon_decay, stop_epsilon, alpha, reward_norm_coef, reward_normalization, running_reward_std, max_grad_norm, attributes, optimizer, optimizer_kwargs, fps, log_frequency)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;66;03m# TODO: figure out: Check if the action space is dictionary type ???\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m episode \u001b[38;5;241m<\u001b[39m num_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# Collect samples, then perform an update on the Q-network\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# Sample a batch from the replay buffer\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     states, actions, _, _, rewards, dones, next_states, batches \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    201\u001b[0m         buffer\u001b[38;5;241m.\u001b[39mgenerate_batches()\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:132\u001b[0m, in \u001b[0;36mdeep_q_learning.<locals>.collect_rollouts\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 132\u001b[0m     \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    134\u001b[0m         state \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mreset(attributes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Running the Training ###\n",
    "\"\"\"\n",
    "Remember:\n",
    "- We can edit parameters for the Learning Algorithm!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Training.Algorithms import deep_q_learning\n",
    "from CoderSchoolAI.Environment.Agent import BasicReplayBuffer, DictReplayBuffer\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "deep_q_learning(\n",
    "    agent=snake_env.snake_agent,\n",
    "    environment=snake_env,\n",
    "    q_network=copy_net,\n",
    "    target_q_network=q_net,\n",
    "    update_target_every=5,\n",
    "    buffer= BasicReplayBuffer(batch_size),\n",
    "    num_episodes=10000,\n",
    "    max_steps_per_episode=100,\n",
    "    epsilon=0.5,\n",
    "    epsilon_decay=0.9999,\n",
    "    stop_epsilon=0.1,\n",
    "    batch_size=batch_size,\n",
    "    reward_norm_coef=0.5,\n",
    "    reward_normalization=True,\n",
    "    alpha=0.005,\n",
    "    attributes=\"game_state\",\n",
    "    log_frequency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the Network ####\n",
    "file_path = \".\"\n",
    "q_net.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How Good Is your Model?\n",
    "from CoderSchoolAI.Environment.Agent import *\n",
    "from CoderSchoolAI.Environment.Shell import *\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "import torch as th\n",
    "\n",
    "snake_env = SnakeEnv(width=8, height=8, cell_size=60, verbose=True, target_fps=8)\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\").copy(), is_module_dict=False,)\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "flatten_size = np.prod(snake_env.get_attribute(\"game_state\").shape)\n",
    "flat_block = FlattenBlock(conv_block.output_size)\n",
    "lin_block = LinearBlock(flat_block.output_size, 16, num_hidden_layers=3, dropout=0.05, hidden_size=[128, 128, 32])\n",
    "\n",
    "out_block = OutputBlock(input_size=lin_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()),)\n",
    "\n",
    "q_net = Net(name='test_q_net_with_game_state')\n",
    "q_net.add_block(input_block)\n",
    "q_net.add_block(conv_block)\n",
    "q_net.add_block(flat_block)\n",
    "q_net.add_block(lin_block)\n",
    "q_net.add_block(out_block)\n",
    "q_net.compile()\n",
    "q_net.load_checkpoint(\"./test_q_net_with_game_state.pt\")\n",
    "\n",
    "def demo_model(\n",
    "    agent: Agent,  # Actor in the Environment\n",
    "    environment: Shell,  # Environment which the Deep Q Network is being trained on \n",
    "    q_network: Net,  # The pretrained Deep Q Network\n",
    "    num_episodes: int = 10,  # Number of episodes to demonstrate\n",
    "    max_steps_per_episode: int = 100,  # Stops the Epsiodic Sampling when the number of steps per episode reaches this value\n",
    "    attributes: Union[str, Tuple[str]] = None,  # attributes to be used for the Network\n",
    "    fps: int = 10,  # Frames per second to run the Agent\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A demonstration of Deep Q Learning with a pretrained Deep Q Network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the Q-network is in evaluation mode\n",
    "    q_network.eval()\n",
    "\n",
    "    # Check if the action space is dictionary type\n",
    "    if isinstance(agent.get_actions(), dict):\n",
    "        raise ValueError(\"The action space for Deep Q Learning cannot be of type Dict.\")\n",
    "\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        environment.clock.tick(fps)\n",
    "        state = environment.reset(attributes)\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps_per_episode:\n",
    "            step += 1\n",
    "            # Get list of possible actions from agent\n",
    "            possible_actions = agent.get_actions()\n",
    "            # Convert state to tensor for feeding into the network\n",
    "            state_tensor = th.tensor(state, dtype=th.float32).to(q_network.device)\n",
    "            state_tensor = th.unsqueeze(state_tensor, 0)\n",
    "            # Feed the state into the q_network to get Q-values for each action\n",
    "            with th.no_grad():  # Disable gradient computation\n",
    "                q_values = q_network(state_tensor)\n",
    "            # Choose action with highest Q-value\n",
    "            _, action_index = th.max(q_values, dim=1)\n",
    "            action = possible_actions[action_index.item()]\n",
    "            # Take action in the environment\n",
    "            next_state, reward, done = environment.step(action, 0, attributes)\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "            environment.render_env()\n",
    "\n",
    "## Demo Function Call:\n",
    "demo_model (\n",
    "    agent=snake_env.snake_agent,\n",
    "    environment=snake_env,\n",
    "    q_network=q_net,\n",
    "    num_episodes=100,\n",
    "    max_steps_per_episode=100,\n",
    "    attributes=\"game_state\",\n",
    "    fps=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoderSchoolAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Network...\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.InputBlock.InputBlock>,\n",
      "\n",
      "Comiled Block:  CoderSchoolAI.Neural.Blocks.ConvBlock.ConvBlock>,\n",
      "  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (8): ReLU()\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (11): ReLU()\n",
      "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): Flatten(start_dim=1, end_dim=-1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### How do We use this in Our Case?\n",
    "\"\"\"\n",
    "On Policy: PPO Agents\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Environment.CoderSchoolEnvironments.SnakeEnvironment import *\n",
    "from CoderSchoolAI.Neural.Blocks import *\n",
    "from CoderSchoolAI.Neural.Net import *\n",
    "from CoderSchoolAI.Neural.ActorCritic.ActorCriticNetwork import *\n",
    "import torch as th\n",
    "\n",
    "snake_env = SnakeEnv(width=16, height=16)\n",
    "input_block = InputBlock(in_attribute=snake_env.get_attribute(\"game_state\"), is_module_dict=False,)\n",
    "conv_block = ConvBlock(input_shape=input_block.in_attribute.space.shape,num_channels=1,depth=5,)\n",
    "# out_block = OutputBlock(input_size=conv_block.output_size, num_classes=len(snake_env.snake_agent.get_actions()),)\n",
    "\n",
    "ppo_net = Net(name='test_ppo_net_with_game_state')\n",
    "ppo_net.add_block(input_block)\n",
    "ppo_net.add_block(conv_block)\n",
    "ppo_net.compile()\n",
    "\n",
    "actor_critic_net = ActorCritic(snake_env[\"game_state\"], snake_env[\"actions\"], ppo_net, net_arch=[32, dict(vf=[32, 32], pi=[32, 32])],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCoderSchoolAI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEnvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicReplayBuffer, DictReplayBuffer\n\u001b[0;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnake_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnake_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnake_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_critic_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_critic_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDictReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:435\u001b[0m, in \u001b[0;36mPPO\u001b[1;34m(agent, environment, actor_critic_net, buffer, num_episodes, max_steps_per_episode, gamma, batch_size, clip_epsilon, alpha, epsilon, entropy_coef, critic_coef, attributes, optimizer, optimizer_kwargs, ppo_epochs, minibatch_size, fps, max_grad_norm, reward_norm_coef, reward_normalization, running_reward_std, log_frequency)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m returns\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m episode \u001b[38;5;241m<\u001b[39m num_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;66;03m# Collect samples, then perform an update on the Q-network\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m     \u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# Sample a batch from the replay buffer\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     states, actions, log_probs, vals, rewards, dones, next_states, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         buffer\u001b[38;5;241m.\u001b[39mgenerate_batches()\n\u001b[0;32m    439\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Training\\Algorithms.py:378\u001b[0m, in \u001b[0;36mPPO.<locals>.collect_rollouts\u001b[1;34m()\u001b[0m\n\u001b[0;32m    375\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_tensor\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Feed the state into the Actor Critic Network\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m probs, actions, vals \u001b[38;5;241m=\u001b[39m \u001b[43mactor_critic_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sample_and_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary Solution to the batched action sampling NEEDS FIX\u001b[39;00m\n\u001b[0;32m    381\u001b[0m a_s \u001b[38;5;241m=\u001b[39m actions\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\ActorCritic\\ActorCriticNetwork.py:202\u001b[0m, in \u001b[0;36mActorCritic.get_sample_and_values\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sample_and_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    Gets Sample Action and Values associated with observations\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    - Param Obs: Actor/Critic Network input tensor\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    - Output[Critic Network (Val Estimation)]: (batch_size, 1)\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     actor_output, critic_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     _action_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    204\u001b[0m     log_probabilities \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\ActorCritic\\ActorCriticNetwork.py:164\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__detatch_features):\n\u001b[1;32m--> 164\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Whether or not to use Gradients on Features Network\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__output_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_net(features))\n\u001b[0;32m    168\u001b[0m     actor_output, critic_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(features), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(features)\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\Net.py:77\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__compiled:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot forward a network that has not been compiled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John\\.conda\\envs\\pytorch\\lib\\site-packages\\CoderSchoolAI\\Neural\\Blocks\\InputBlock.py:126\u001b[0m, in \u001b[0;36mInputBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\n\u001b[1;32m--> 126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_connections \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_connections, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_connections) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    130\u001b[0m ):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "### Running the Training ###\n",
    "\"\"\"\n",
    "Remember:\n",
    "- We can edit parameters for the Learning Algorithm!\n",
    "\"\"\"\n",
    "from CoderSchoolAI.Training.Algorithms import PPO\n",
    "from CoderSchoolAI.Environment.Agent import BasicReplayBuffer, DictReplayBuffer\n",
    "batch_size = 512\n",
    "\n",
    "PPO(\n",
    "    agent=snake_env.snake_agent,\n",
    "    environment=snake_env,\n",
    "    actor_critic_net=actor_critic_net,\n",
    "    buffer=DictReplayBuffer(batch_size),\n",
    "    num_episodes=10000,\n",
    "    max_steps_per_episode=100,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
